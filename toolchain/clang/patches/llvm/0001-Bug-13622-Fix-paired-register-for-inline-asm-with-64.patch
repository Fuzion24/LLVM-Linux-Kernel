From fbf57e82a728396a310cd31ec63598a4d1e34b34 Mon Sep 17 00:00:00 2001
From: Weiming Zhao <weimingz@codeaurora.org>
Date: Tue, 21 Aug 2012 14:04:25 -0700
Subject: [PATCH] Bug 13622: Fix paired register for inline asm with 64-bit
 data on ARM

---
 include/llvm/Target/TargetLowering.h             |   15 +++++++++
 lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp |   22 +++++++++----
 lib/Target/ARM/ARMISelLowering.cpp               |   36 ++++++++++++++++++++--
 lib/Target/ARM/ARMISelLowering.h                 |    4 ++-
 test/CodeGen/ARM/inlineasm-64bit.ll              |   20 ++++++++++++
 test/CodeGen/ARM/thumb1-inlineasm-64bit.ll       |   20 ++++++++++++
 6 files changed, 106 insertions(+), 11 deletions(-)
 create mode 100644 test/CodeGen/ARM/inlineasm-64bit.ll
 create mode 100644 test/CodeGen/ARM/thumb1-inlineasm-64bit.ll

--- llvm.orig/include/llvm/Target/TargetLowering.h
+++ llvm/include/llvm/Target/TargetLowering.h
@@ -29,6 +29,7 @@
 #include "llvm/IR/Attributes.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/IR/InlineAsm.h"
+#include "llvm/ADT/SmallSet.h"
 #include "llvm/Support/CallSite.h"
 #include "llvm/Support/DebugLoc.h"
 #include "llvm/Target/TargetCallingConv.h"
@@ -1650,6 +1651,20 @@
     getRegForInlineAsmConstraint(const std::string &Constraint,
                                  EVT VT) const;
 
+  /// getConstraintType - Given a constraint, return the type of constraint it
+  /// is for this target.
+  /// AssignedPhyRegs is used to keep pre-specified physical regs for the
+  /// inline ASM. This interface can be used to allocate physical registers and
+  /// avoid conflicting with pre-specified physical registers.
+  virtual std::pair<unsigned, const TargetRegisterClass*>
+    getRegForInlineAsmConstraint(const std::string &Constraint,
+                                 EVT VT,
+                                 const SmallSet<unsigned, 32> &AssignedPhyRegs
+                                ) const {
+      // The default implementation simply ignores the AssignedPhyRegs.
+      return getRegForInlineAsmConstraint(Constraint, VT);
+    }
+
   /// LowerXConstraint - try to replace an X constraint, which matches anything,
   /// with another that has more specific requirements based on the type of the
   /// corresponding operand.  This returns null if there is no replacement to
--- llvm.orig/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
+++ llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp
@@ -5729,7 +5729,8 @@
 static void GetRegistersForValue(SelectionDAG &DAG,
                                  const TargetLowering &TLI,
                                  DebugLoc DL,
-                                 SDISelAsmOperandInfo &OpInfo) {
+                                 SDISelAsmOperandInfo &OpInfo,
+                                 SmallSet<unsigned, 32> &AssignedPhyRegs) {
   LLVMContext &Context = *DAG.getContext();
 
   MachineFunction &MF = DAG.getMachineFunction();
@@ -5739,7 +5740,8 @@
   // register class, find it.
   std::pair<unsigned, const TargetRegisterClass*> PhysReg =
     TLI.getRegForInlineAsmConstraint(OpInfo.ConstraintCode,
-                                     OpInfo.ConstraintVT);
+                                     OpInfo.ConstraintVT,
+                                     AssignedPhyRegs);
 
   unsigned NumRegs = 1;
   if (OpInfo.ConstraintVT != MVT::Other) {
@@ -5788,6 +5790,7 @@
 
     // This is a explicit reference to a physical register.
     Regs.push_back(AssignedReg);
+    AssignedPhyRegs.insert(AssignedReg);
 
     // If this is an expanded reference, add the rest of the regs to Regs.
     if (NumRegs != 1) {
@@ -5800,6 +5803,7 @@
       for (; NumRegs; --NumRegs, ++I) {
         assert(I != RC->end() && "Ran out of registers to allocate!");
         Regs.push_back(*I);
+        AssignedPhyRegs.insert(*I);
       }
     }
 
@@ -5830,6 +5834,8 @@
 ///
 void SelectionDAGBuilder::visitInlineAsm(ImmutableCallSite CS) {
   const InlineAsm *IA = cast<InlineAsm>(CS.getCalledValue());
+  /// Physical regs assigned to this inline ASM.
+  SmallSet<unsigned, 32> AssignedPhyRegs;
 
   /// ConstraintOperands - Information about all of the constraints.
   SDISelAsmOperandInfoVector ConstraintOperands;
@@ -5929,10 +5935,12 @@
       if (OpInfo.ConstraintVT != Input.ConstraintVT) {
         std::pair<unsigned, const TargetRegisterClass*> MatchRC =
           TLI.getRegForInlineAsmConstraint(OpInfo.ConstraintCode,
-                                           OpInfo.ConstraintVT);
+                                           OpInfo.ConstraintVT,
+                                           AssignedPhyRegs);
         std::pair<unsigned, const TargetRegisterClass*> InputRC =
           TLI.getRegForInlineAsmConstraint(Input.ConstraintCode,
-                                           Input.ConstraintVT);
+                                           Input.ConstraintVT,
+                                           AssignedPhyRegs);
         if ((OpInfo.ConstraintVT.isInteger() !=
              Input.ConstraintVT.isInteger()) ||
             (MatchRC.second != InputRC.second)) {
@@ -5994,9 +6002,9 @@
     }
 
     // If this constraint is for a specific register, allocate it before
-    // anything else.
+    // anything else. AssignedPhyRegs tracks those allocated registers.
     if (OpInfo.ConstraintType == TargetLowering::C_Register)
-      GetRegistersForValue(DAG, TLI, getCurDebugLoc(), OpInfo);
+      GetRegistersForValue(DAG, TLI, getCurDebugLoc(), OpInfo, AssignedPhyRegs);
   }
 
   // Second pass - Loop over all of the operands, assigning virtual or physregs
@@ -6007,7 +6015,7 @@
     // C_Register operands have already been allocated, Other/Memory don't need
     // to be.
     if (OpInfo.ConstraintType == TargetLowering::C_RegisterClass)
-      GetRegistersForValue(DAG, TLI, getCurDebugLoc(), OpInfo);
+      GetRegistersForValue(DAG, TLI, getCurDebugLoc(), OpInfo, AssignedPhyRegs);
   }
 
   // AsmNodeOperands - The operands for the ISD::INLINEASM node.
--- llvm.orig/lib/Target/ARM/ARMISelLowering.cpp
+++ llvm/lib/Target/ARM/ARMISelLowering.cpp
@@ -10018,8 +10018,9 @@
 
 typedef std::pair<unsigned, const TargetRegisterClass*> RCPair;
 RCPair
-ARMTargetLowering::getRegForInlineAsmConstraint(const std::string &Constraint,
-                                                EVT VT) const {
+ARMTargetLowering::getRegForInlineAsmConstraint(
+    const std::string &Constraint, EVT VT,
+    const SmallSet<unsigned, 32> &AssignedPhyRegs) const {
   if (Constraint.size() == 1) {
     // GCC ARM Constraint Letters
     switch (Constraint[0]) {
@@ -10032,6 +10033,34 @@
         return RCPair(0U, &ARM::hGPRRegClass);
       break;
     case 'r':
+      // For 64-bit data passing (e.g. used by ldrexd/strexd), ARM ABI requires
+      // a (even,even+1) register pair. Since LLVM does not support register
+      // pair constraints now, we hard code physical registers here, which is
+      // similar to the implementation of the ldrexd intrinsics (see
+      // ARMTargetLowering::EmitAtomicBinary64).
+      // (Note: although Thumb allows any two GPRs in the register pair, in
+      // order to properly support %H modifier that specifies the odd register
+      // of a pair and is often used together with ldrexd/strexd, we always
+      // enforce a (even, even+1) register pair.)
+      // FIXME: When LLVM supports paired register class for 64-bit data in
+      // future, we should just return that register class here and let register
+      // allocator to assign physical registers.
+      if (VT.getSizeInBits() == 64)  {
+        // The valid registers for ldrexd/strexd is r0-r13 for ARM. But we skip
+        // the pairs using FP(r7 on Darwin, r11 on Linux) and SP(r13) for
+        // safety. So we use ARM::R12 as sentinel value.
+        unsigned AvailRegs[] = {ARM::R0, ARM::R1, ARM::R2, ARM::R3,
+                                ARM::R4, ARM::R5, ARM::R8, ARM::R9, ARM::R12};
+
+        for (unsigned i = 0; AvailRegs[i] != ARM::R12; i += 2) {
+          unsigned RegEven = AvailRegs[i], RegOdd = AvailRegs[i+1];
+          if (RegOdd == (Subtarget->isThumb() ? ARM::R7 : ARM::R11))
+            continue;
+          if (!AssignedPhyRegs.count(RegEven) && !AssignedPhyRegs.count(RegOdd))
+            return RCPair(RegEven, &ARM::GPRRegClass);
+        }
+        return RCPair(0U, NULL);
+      }
       return RCPair(0U, &ARM::GPRRegClass);
     case 'w':
       if (VT == MVT::f32)
@@ -10058,7 +10087,8 @@
   if (StringRef("{cc}").equals_lower(Constraint))
     return std::make_pair(unsigned(ARM::CPSR), &ARM::CCRRegClass);
 
-  return TargetLowering::getRegForInlineAsmConstraint(Constraint, VT);
+  return TargetLowering::getRegForInlineAsmConstraint(Constraint, VT,
+                                                      AssignedPhyRegs);
 }
 
 /// LowerAsmOperandForConstraint - Lower the specified operand into the Ops
--- llvm.orig/lib/Target/ARM/ARMISelLowering.h
+++ llvm/lib/Target/ARM/ARMISelLowering.h
@@ -347,9 +347,11 @@
     ConstraintWeight getSingleConstraintMatchWeight(
       AsmOperandInfo &info, const char *constraint) const;
 
+    using TargetLowering::getRegForInlineAsmConstraint;
     std::pair<unsigned, const TargetRegisterClass*>
       getRegForInlineAsmConstraint(const std::string &Constraint,
-                                   EVT VT) const;
+                           EVT VT,
+                           const SmallSet<unsigned, 32> &AssignedPhyRegs) const;
 
     /// LowerAsmOperandForConstraint - Lower the specified operand into the Ops
     /// vector.  If it is invalid, don't add anything to Ops. If hasMemory is
--- /dev/null
+++ llvm/test/CodeGen/ARM/inlineasm-64bit.ll
@@ -0,0 +1,20 @@
+; RUN: llc < %s -O3 -march=arm | FileCheck %s
+
+define void @i64_write(i64* %p, i64 %val) nounwind {
+; CHECK: i64_write
+; CHECK: ldrexd [[REG1:(r[0-9]?[02468])]], {{r[0-9]?[13579]}}, [r{{[0-9]+}}]
+; CHECK: strexd [[REG1]], {{r[0-9]?[02468]}}, {{r[0-9]?[13579]}}
+  %1 = tail call i64 asm sideeffect "1: ldrexd $0, ${0:H}, [$2]\0A strexd $0, $3, ${3:H}, [$2]\0A teq $0, #0\0A bne 1b", "=&r,=*Qo,r,r,~{cc}"(i64* %p, i64* %p, i64 %val) nounwind
+  ret void
+}
+
+; check if callee-saved registers used by inline asm are saved/restored
+define void @foo(i64* %p, i64 %i) nounwind {
+; CHECK: foo
+; CHECK: push {{{r[4-9]|r10|r11}}
+; CHECK: ldrexd [[REG1:(r[0-9]?[02468])]], {{r[0-9]?[13579]}}, [r{{[0-9]+}}]
+; CHECK: strexd [[REG1]], {{r[0-9]?[02468]}}, {{r[0-9]?[13579]}}
+; CHECK: pop {{{r[4-9]|r10|r11}}
+  %1 = tail call { i64, i64 } asm sideeffect "@ atomic64_set\0A1:\09ldrexd\09$0, ${0:H}, [$3]\0Aldrexd\09$1, ${1:H}, [$3]\0A\09strexd\09$0, $4, ${4:H}, [$3]\0A\09teq\09$0, #0\0A\09bne\091b", "=&r,=&r,=*Qo,r,r,~{cc}"(i64* %p, i64* %p, i64 %i) nounwind
+  ret void
+}
--- /dev/null
+++ llvm/test/CodeGen/ARM/thumb1-inlineasm-64bit.ll
@@ -0,0 +1,20 @@
+; RUN: llc < %s -O3 -march=thumb | FileCheck %s
+
+define void @i64_write(i64* %p, i64 %val) nounwind {
+; CHECK: i64_write
+; CHECK: ldrexd [[REG1:(r[0-9]?[02468])]], {{r[0-9]?[13579]}}, [r{{[0-9]+}}]
+; CHECK: strexd [[REG1]], {{r[0-9]?[02468]}}, {{r[0-9]?[13579]}}
+  %1 = tail call i64 asm sideeffect "1: ldrexd $0, ${0:H}, [$2]\0A strexd $0, $3, ${3:H}, [$2]\0A teq $0, #0\0A bne 1b", "=&r,=*Qo,r,r,~{cc}"(i64* %p, i64* %p, i64 %val) nounwind
+  ret void
+}
+
+; check if callee-saved registers used by inline asm are saved/restored
+define void @foo(i64* %p, i64 %i) nounwind {
+; CHECK: foo
+; CHECK: push {{{r[4-9]|r10|r11}}
+; CHECK: ldrexd [[REG1:(r[0-9]?[02468])]], {{r[0-9]?[13579]}}, [r{{[0-9]+}}]
+; CHECK: strexd [[REG1]], {{r[0-9]?[02468]}}, {{r[0-9]?[13579]}}
+; CHECK: pop {{{r[4-9]|r10|r11}}
+  %1 = tail call { i64, i64 } asm sideeffect "@ atomic64_set\0A1:\09ldrexd\09$0, ${0:H}, [$3]\0Aldrexd\09$1, ${1:H}, [$3]\0A\09strexd\09$0, $4, ${4:H}, [$3]\0A\09teq\09$0, #0\0A\09bne\091b", "=&r,=&r,=*Qo,r,r,~{cc}"(i64* %p, i64* %p, i64 %i) nounwind
+  ret void
+}
