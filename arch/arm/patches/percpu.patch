See http://kernel.opensuse.org/cgit/kernel/commit/?id=509eb76ebf97

The use of sp as a register variable is not supported in clang and
the behavior they are working around in GCC may not even be present 
in clang.

From the link above:

Unfortunately, GCC doesn't treat a "memory" clobber on a non-volatile
asm block as a side-effect, and will happily re-order it before other
memory clobbers (including those in prempt_disable()) and cache the
value. This has been observed to break the cmpxchg logic in the slub
allocator, leading to livelock in kmem_cache_alloc in mainline kernels.

Because the GCC workaround causes numerous warnings, revert the change
until it can be verified that it is in fact an issue for clang.


Signed-off-by: Mark Charlebois <charlebm@gmail.com>
---

diff --git a/arch/arm/include/asm/compiler.h b/arch/arm/include/asm/compiler.h
index 8155db2..715af47 100644
--- a/arch/arm/include/asm/compiler.h
+++ b/arch/arm/include/asm/compiler.h
@@ -11,5 +11,35 @@
  */
 #define __asmeq(x, y)  ".ifnc " x "," y " ; .err ; .endif\n\t"
 
+#if defined(CONFIG_SMP) && !defined(CONFIG_CPU_V6)
+/*
+ * Read TPIDRPRW.
+ * GCC requires a workaround as it does not treat a "memory" clobber on a 
+ * non-volatile asm block as a side-effect.
+ * We want to allow caching the value, so for GCC avoid using volatile and
+ * instead use a fake stack read to hazard against barrier().
+ */
+#if defined(__clang__)
+static inline unsigned long read_TPIDRPRW() {
+	unsigned long off;
+
+	/*
+	 * Read TPIDRPRW.
+	 */
+	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off) : : "memory");
+
+	return off;
+}
+#else 
+static inline unsigned long read_TPIDRPRW() {
+	unsigned long off;
+	register unsigned long *sp asm ("sp");
+
+	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off) : "Q" (*sp));
+
+	return off;
+}
+#endif
+#endif
 
 #endif /* __ASM_ARM_COMPILER_H */
diff --git a/arch/arm/include/asm/percpu.h b/arch/arm/include/asm/percpu.h
index 209e650..5a76f3b 100644
--- a/arch/arm/include/asm/percpu.h
+++ b/arch/arm/include/asm/percpu.h
@@ -16,6 +16,7 @@
 #ifndef _ASM_ARM_PERCPU_H_
 #define _ASM_ARM_PERCPU_H_
 
+
 /*
  * Same as asm-generic/percpu.h, except that we store the per cpu offset
  * in the TPIDRPRW. TPIDRPRW only exists on V6K and V7
@@ -27,21 +28,10 @@ static inline void set_my_cpu_offset(unsigned long off)
 	asm volatile("mcr p15, 0, %0, c13, c0, 4" : : "r" (off) : "memory");
 }
 
-static inline unsigned long __my_cpu_offset(void)
-{
-	unsigned long off;
-	register unsigned long *sp asm ("sp");
+#include "asm/compiler.h"
 
-	/*
-	 * Read TPIDRPRW.
-	 * We want to allow caching the value, so avoid using volatile and
-	 * instead use a fake stack read to hazard against barrier().
-	 */
-	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off) : "Q" (*sp));
+#define __my_cpu_offset read_TPIDRPRW()
 
-	return off;
-}
-#define __my_cpu_offset __my_cpu_offset()
 #else
 #define set_my_cpu_offset(x)	do {} while(0)
 
