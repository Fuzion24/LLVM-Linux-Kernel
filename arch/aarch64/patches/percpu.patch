diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h
index 453a179..5d51078 100644
--- a/arch/arm64/include/asm/percpu.h
+++ b/arch/arm64/include/asm/percpu.h
@@ -18,25 +18,14 @@
 
 #ifdef CONFIG_SMP
 
+#include "asm/compiler.h"
+
 static inline void set_my_cpu_offset(unsigned long off)
 {
 	asm volatile("msr tpidr_el1, %0" :: "r" (off) : "memory");
 }
 
-static inline unsigned long __my_cpu_offset(void)
-{
-	unsigned long off;
-	register unsigned long *sp asm ("sp");
-
-	/*
-	 * We want to allow caching the value, so avoid using volatile and
-	 * instead use a fake stack read to hazard against barrier().
-	 */
-	asm("mrs %0, tpidr_el1" : "=r" (off) : "Q" (*sp));
-
-	return off;
-}
-#define __my_cpu_offset __my_cpu_offset()
+#define __my_cpu_offset read_TPIDRPRW()
 
 #else	/* !CONFIG_SMP */
 
diff --git a/arch/arm64/include/asm/compiler.h b/arch/arm64/include/asm/compiler.h
index ee35fd0..760a455 100644
--- a/arch/arm64/include/asm/compiler.h
+++ b/arch/arm64/include/asm/compiler.h
@@ -27,4 +27,37 @@
  */
 #define __asmeq(x, y)  ".ifnc " x "," y " ; .err ; .endif\n\t"
 
+#if defined(CONFIG_SMP) && !defined(CONFIG_CPU_V6)
+/*
+ * Read TPIDRPRW.
+ * GCC requires a workaround as it does not treat a "memory" clobber on a
+ * non-volatile asm block as a side-effect.
+ * We want to allow caching the value, so for GCC avoid using volatile and
+ * instead use a fake stack read to hazard against barrier().
+ */
+#if defined(__clang__)
+static inline unsigned long read_TPIDRPRW(void)
+{
+	unsigned long off = 1;
+
+	/*
+	 * Read TPIDRPRW.
+	 */
+	// FIXME: asm("mrs %0, tpidr_el1" : "=r" (off) : : "memory");
+
+	return off;
+}
+#else
+static inline unsigned long read_TPIDRPRW(void)
+{
+	unsigned long off;
+	register unsigned long *sp asm ("sp");
+
+	asm("mrs %0, tpidr_el1" : "=r" (off) : "Q" (*sp));
+
+	return off;
+}
+#endif
+#endif
+
 #endif	/* __ASM_COMPILER_H */
